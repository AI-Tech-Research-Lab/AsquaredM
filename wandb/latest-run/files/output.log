05/31 03:17:12 PM gpu device = 0
05/31 03:17:12 PM args = Namespace(data='../datasets/cifar10', dataset='cifar10', batch_size=64, learning_rate=0.025, learning_rate_min=0.001, momentum=0.9, weight_decay=0.0003, report_freq=50, gpu=0, epochs=100, init_channels=16, layers=8, model_path='saved_models', cutout=False, cutout_length=16, cutout_prob=1.0, save='../experiments/nasbench201/cifar10/search-exp-20240531-151710-2-5534', seed=2, grad_clip=5, train_portion=0.5, unrolled=False, arch_learning_rate=0.0003, arch_weight_decay=0.001, perturb_alpha='none', epsilon_alpha=0.3, wandb=True, nasbench=True)
05/31 03:17:13 PM param size = 1.686136MB
Files already downloaded and verified
num_train = 50000 split_train = 25000 split_valid = 25000
05/31 03:17:16 PM epoch 0 lr 2.498816e-02
05/31 03:17:16 PM epoch 0 epsilon_alpha 3.000000e-02
05/31 03:17:16 PM genotype = [('avg_pool_3x3', 0), ('nor_conv_3x3', 0), ('none', 1), ('avg_pool_3x3', 0), ('nor_conv_1x1', 1), ('none', 2)]
arch-parameters :
tensor([[0.2000, 0.1998, 0.2001, 0.1997, 0.2004],
        [0.1999, 0.1998, 0.2000, 0.2002, 0.2001],
        [0.2002, 0.1998, 0.1999, 0.2001, 0.2000],
        [0.2000, 0.1999, 0.2001, 0.1999, 0.2001],
        [0.1998, 0.1999, 0.2003, 0.1999, 0.2000],
        [0.2001, 0.2000, 0.1999, 0.2001, 0.1999]])
/home/gambella/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/gambella/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
05/31 03:17:18 PM train 000 2.345711e+00 6.250000 54.687500
05/31 03:17:37 PM train 050 2.092995e+00 21.292892 73.621323
05/31 03:17:57 PM train 100 2.002135e+00 24.118193 78.326111
05/31 03:18:17 PM train 150 1.954976e+00 25.651903 80.329056
05/31 03:18:42 PM train 200 1.916757e+00 27.215485 81.724190
05/31 03:19:07 PM train 250 1.885646e+00 28.280628 82.800049
05/31 03:19:31 PM train 300 1.858767e+00 29.396801 83.528862
05/31 03:19:56 PM train 350 1.837533e+00 30.172720 84.058937
05/31 03:20:16 PM train_acc 30.920000
05/31 03:20:16 PM valid 000 1.599697e+00 43.750000 84.375000
05/31 03:20:20 PM valid 050 1.724914e+00 35.232845 88.204659
05/31 03:20:22 PM valid 100 1.717462e+00 35.937500 88.768562
05/31 03:20:24 PM valid 150 1.719508e+00 35.854717 88.451988
05/31 03:20:26 PM valid 200 1.708984e+00 36.466106 88.502800
05/31 03:20:29 PM valid 250 1.713571e+00 36.379482 88.427544
05/31 03:20:31 PM valid 300 1.717246e+00 36.290489 88.424004
05/31 03:20:33 PM valid 350 1.715475e+00 36.449429 88.452637
05/31 03:20:35 PM valid_acc 36.343998
05/31 03:20:36 PM Best model found at epoch 0
/home/gambella/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
05/31 03:20:36 PM epoch 1 lr 2.495858e-02
05/31 03:20:36 PM epoch 1 epsilon_alpha 3.270000e-02
05/31 03:20:36 PM genotype = [('nor_conv_1x1', 0), ('skip_connect', 0), ('nor_conv_1x1', 1), ('skip_connect', 0), ('skip_connect', 1), ('skip_connect', 2)]
arch-parameters :
tensor([[0.1996, 0.2013, 0.2031, 0.1991, 0.1970],
        [0.1987, 0.2036, 0.2007, 0.1975, 0.1995],
        [0.2001, 0.1997, 0.2022, 0.2010, 0.1971],
        [0.1950, 0.2050, 0.2009, 0.1976, 0.2016],
        [0.1991, 0.2029, 0.2003, 0.1977, 0.2000],
        [0.1985, 0.2028, 0.2014, 0.1983, 0.1990]])
